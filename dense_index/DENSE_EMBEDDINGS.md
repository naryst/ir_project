## План того, что нужно сделать:

1. Взять готовую модель [SigLip2](https://huggingface.co/google/siglip2-base-patch16-224) и разобраться, как с ее помощью получать ембеддинги. Она мультимодальная, и будет работать как для текста, так и для картинок. При этом картинки и текст с одинаковым смыслом будут иметь близкие ембеддинги.
2. Прогнать модель на текстах. Важно проверить, что количество токенов после токенизации текстов помещается в контекстное окно модели. Если оно не помещается, то есть два возможных варианта решения:
   1. Уменьшить текст. Например можно суммаризировать его через https://huggingface.co/itqop/gemma-2-2b-summarize-adapter 
   2. Взять версию SigLip побольше, например эту https://huggingface.co/google/siglip2-base-patch16-512 
3. Прогнать ее на картинках и получить ембеддинги. Тут проблем быть не должно, просто сжать картинку до нужных размеров (на которой модель училась) перед подачей. По идее такое умеет стандартный Processor
4. Засунуть пары (ембеддинг, id) в [FAISS](https://github.com/facebookresearch/faiss?tab=readme-ov-file) и сбилдить его. Желательно еще в id как то учитывать, что это за ембеддинг, картинки или текста. Это нужно, чтобы собрать статистику, что оказалось полезнее.
5. Возможно просто заюзать FAISS не прокатит, и нужно будет сделать на коленке свой поиск по векторам. 
6. Обновить демку в StreamLit. Либо создать новую, если эта не особо расширяемая под новый вид поиска.  